# [Apache Spark Intro](https://spark.apache.org/)
* **What is Apache Spark?**

   Apache Spark is an open-source cluster-computing framework(wiki). It is a general-purpose Big data processing engine, suitable for use in a wide range of circumstances. It allows data workers to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to datasets.  it works with the filesystem to **distribute**  data across the cluster, and process that data in **parallel**. **Spark jobs perform multiple operations consecutively, in memory and only
spilling to disk when required by memory limitations.**
    * [Chapter One](http://www.bigdata-toronto.com/2016/assets/getting_started_with_apache_spark.pdf)
    
* **Why  Apache Spark??**
    * [Spark is mostly popular for its speed,ease of use,capability of running evereywhere ](https://spark.apache.org/) 
    * It is much more faster than MapReduce
    * [Use case](https://hortonworks.com/apache/spark/#section_2)
* **Spark is not the alternative of Hadoop. It is the alternative of MapReduce**
* [Spark Ecosystem](https://www.kdnuggets.com/2016/03/top-spark-ecosystem-projects.html)


![alt text][SparkEcosystem]















[SparkEcosystem]: https://github.com/Shayokh144/Spark_with_Python-Big_data_Fastest-Smallest_solution-/blob/master/sparkEco.png
