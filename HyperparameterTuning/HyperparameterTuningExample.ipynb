{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Background](https://cloud.google.com/ml-engine/docs/hyperparameter-tuning-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tuning In Spark](https://spark.apache.org/docs/2.1.0/ml-tuning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/asif/spark-2.1.0-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession # to load dataframe we need this\n",
    "spark = SparkSession.builder.appName('titanicRandomForestClassifier').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawData = spark.read.csv('TitanicData.csv', inferSchema=True, header=True)\n",
    "rawData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in rawData = 891\n",
      "Number of rows in each column:\n",
      " [Row(summary='count', PassengerId='891', Survived='891', Pclass='891', Name='891', Sex='891', Age='714', SibSp='891', Parch='891', Ticket='891', Fare='891', Cabin='204', Embarked='889')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "totalNumberOfRow = rawData.count()\n",
    "print('Number of rows in rawData = {0}'.format(totalNumberOfRow))\n",
    "collectNumRowsInEachCol = rawData.describe().filter(col(\"summary\") == \"count\").collect()\n",
    "print(\"Number of rows in each column:\\n\",collectNumRowsInEachCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Age', 'Cabin', 'Embarked' have missing values\n",
    "\n",
    "## Working with 'Embarked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Embarked|\n",
      "+--------+\n",
      "|       Q|\n",
      "|    null|\n",
      "|       C|\n",
      "|       S|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawData.select('Embarked').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       Q|   77|\n",
      "|    null|    2|\n",
      "|       C|  168|\n",
      "|       S|  644|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawData.groupBy(\"Embarked\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = rawData.na.fill('missing',subset=['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "indexer = StringIndexer(inputCol='Embarked', outputCol='EmbarkedInt')\n",
    "# for multiple column pipeline should be used\n",
    "rawData = indexer.fit(rawData).transform(rawData)\n",
    "encoder = OneHotEncoder(inputCol=\"EmbarkedInt\", outputCol=\"categoryEmbarked\")\n",
    "rawData = encoder.transform(rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=1, Survived=0, Pclass=3, Name='Braund, Mr. Owen Harris', Sex='male', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin=None, Embarked='S', EmbarkedInt=0.0, categoryEmbarked=SparseVector(3, {0: 1.0}))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.select('Cabin').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = rawData.drop('Cabin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for mean and standerd dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMean(dataframe, colName):\n",
    "    from pyspark.sql.functions import mean\n",
    "    meanVal = dataframe.select(mean(dataframe[colName])).collect()\n",
    "    meanVal = meanVal[0][0]\n",
    "    return meanVal\n",
    "\n",
    "def getStddev(dataframe, colName):\n",
    "    from pyspark.sql.functions import stddev\n",
    "    stddevVal = dataframe.select(stddev(dataframe[colName])).collect()\n",
    "    stddevVal = stddevVal[0][0]\n",
    "    return stddevVal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanAge  = 29.69911764705882   \n",
      "stddevAge = 14.526497332334035\n"
     ]
    }
   ],
   "source": [
    "\n",
    "meanAge = getMean(rawData, 'Age')\n",
    "stddevAge = getStddev(rawData, 'Age')\n",
    "print('meanAge  = {0}   \\nstddevAge = {1}'.format(meanAge, stddevAge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               Age|\n",
      "+-------+------------------+\n",
      "|  count|               891|\n",
      "|   mean|29.699117647058763|\n",
      "| stddev|13.002015226002891|\n",
      "|    min|              0.42|\n",
      "|    max|              80.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawData = rawData.na.fill(meanAge, ['Age'])\n",
    "rawData.describe('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: double (nullable = false)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = false)\n",
      " |-- EmbarkedInt: double (nullable = true)\n",
      " |-- categoryEmbarked: vector (nullable = true)\n",
      " |-- SexInt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol='Sex', outputCol='SexInt')\n",
    "# for multiple column pipeline should be used\n",
    "rawData = indexer.fit(rawData).transform(rawData)\n",
    "rawData = rawData.drop('Sex')\n",
    "rawData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|FareByPclass|\n",
      "+------------+\n",
      "|        2.42|\n",
      "|       71.28|\n",
      "|        2.64|\n",
      "|        53.1|\n",
      "|        2.68|\n",
      "|        2.82|\n",
      "|       51.86|\n",
      "|        7.02|\n",
      "|        3.71|\n",
      "|       15.04|\n",
      "|        5.57|\n",
      "|       26.55|\n",
      "|        2.68|\n",
      "|       10.42|\n",
      "|        2.62|\n",
      "|         8.0|\n",
      "|        9.71|\n",
      "|         6.5|\n",
      "|         6.0|\n",
      "|        2.41|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding two columns value\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "def findSum(Parch, SibSp):\n",
    "    sum_ = float(SibSp) + float(Parch)\n",
    "    return sum_\n",
    "    \n",
    "userDefiendFuncForSum =  udf(findSum, DoubleType())\n",
    "rawData = rawData.withColumn('sumPS', userDefiendFuncForSum(col('Parch'),col('SibSp')))\n",
    "\n",
    "\n",
    "\n",
    "def FareBysumPS(fare, sumPS):\n",
    "    ans = float(fare) / (float(sumPS)+1)\n",
    "    return ans\n",
    "    \n",
    "userDefiendFuncForFareBysumPS =  udf(FareBysumPS, DoubleType())\n",
    "rawData = rawData.withColumn('FareBysumPS', userDefiendFuncForFareBysumPS(col('Fare'),col('sumPS')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getFareByPclass(fare,pclass):\n",
    "    fare = float(fare)/(float(pclass))\n",
    "    fare = float(\"{0:.2f}\".format(fare))\n",
    "    return fare\n",
    "                        \n",
    "userDefiendFuncForFareByPclass =  udf(getFareByPclass, DoubleType())\n",
    "rawData = rawData.withColumn('FareByPclass', userDefiendFuncForFareByPclass(col('Fare'),col('Pclass')))\n",
    "rawData.select('FareByPclass').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'label', 'Pclass', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked', 'EmbarkedInt', 'categoryEmbarked', 'SexInt', 'sumPS', 'FareBysumPS', 'FareByPclass']\n"
     ]
    }
   ],
   "source": [
    "rawData = rawData.withColumnRenamed(\"Survived\", \"label\")\n",
    "print(rawData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols =[ 'Pclass','SibSp', 'Parch', 'Fare','categoryEmbarked',\n",
    "                                         'SexInt','sumPS','FareBysumPS'],\n",
    "                            outputCol = 'features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=1, label=0, Pclass=3, Name='Braund, Mr. Owen Harris', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Embarked='S', EmbarkedInt=0.0, categoryEmbarked=SparseVector(3, {0: 1.0}), SexInt=0.0, sumPS=1.0, FareBysumPS=3.625, FareByPclass=2.42, features=DenseVector([3.0, 1.0, 0.0, 7.25, 1.0, 0.0, 0.0, 0.0, 1.0, 3.625]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFromAssembler = assembler.transform(rawData)\n",
    "dataFromAssembler.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "rfc = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",numTrees=500,maxDepth=15, maxBins=32, minInstancesPerNode=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalData = dataFromAssembler.select('features','label')\n",
    "trainData, testData = finalData.randomSplit([0.7,0.3])\n",
    "rfc_model = rfc.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC area under the curve= 0.8759426847662144\n",
      "rfc acc=  0.8081180811808119\n"
     ]
    }
   ],
   "source": [
    "rfc_preds = rfc_model.transform(testData)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "rfc_eval = BinaryClassificationEvaluator(labelCol='label')\n",
    "print('RFC area under the curve=',rfc_eval.evaluate(rfc_preds))\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval = MulticlassClassificationEvaluator(labelCol='label', metricName='accuracy')\n",
    "rfc_acc = acc_eval.evaluate(rfc_preds)\n",
    "print('rfc acc= ',rfc_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_preds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       0.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_preds.select('label','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><h1>Now we try hypermeter tuning</h1></span>\n",
    "# <span style=\"color:blue\"><h2><a href=\"https://spark.apache.org/docs/2.1.0/ml-tuning.html#cross-validation\">Cross-Validation</a></h2></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "rfcNew = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", minInstancesPerNode=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import  ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rfcNew.numTrees, [40, 100, 200,300,500]) \\\n",
    "    .addGrid(rfcNew.maxDepth, [10,15,20,28]) \\\n",
    "    .addGrid(rfcNew.maxBins, [8,16,32]) \\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "crossval = CrossValidator(estimator=rfcNew,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10)  # use 3+ folds in practice\n",
    "cvModel = crossval.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (\n",
    "prediction = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC area under the curve= 0.873912286808215\n",
      "rfc acc after tuning=  0.8228782287822878\n"
     ]
    }
   ],
   "source": [
    "rfc_eval_new = BinaryClassificationEvaluator(labelCol='label')\n",
    "print('RFC area under the curve=',rfc_eval_new.evaluate(prediction))\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval_new = MulticlassClassificationEvaluator(labelCol='label', metricName='accuracy')\n",
    "rfc_acc = acc_eval_new.evaluate(prediction)\n",
    "print('rfc acc after tuning= ',rfc_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel (uid=rfc_edf5710b2ff4) with 100 trees"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\"><h2><a href=\"https://spark.apache.org/docs/2.1.0/ml-tuning.html#train-validation-split\">Train-Validation Split</a></h2></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcTrainValidationSplit = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",maxBins=16, minInstancesPerNode=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGridTrainValidationSplit = ParamGridBuilder() \\\n",
    "    .addGrid(rfcNew.numTrees, [20,32,64, 100, 200,300,500]) \\\n",
    "    .addGrid(rfcNew.maxDepth, [10,15,20,28]) \\\n",
    "    .build()\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit(estimator=rfcTrainValidationSplit,\n",
    "                           estimatorParamMaps=paramGridTrainValidationSplit,\n",
    "                           evaluator=BinaryClassificationEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsModel = tvs.fit(trainData)\n",
    "tvsPrediction = tvsModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvsPrediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC area under the curve= 0.8934331128901266\n",
      "rfc acc after tuning=  0.8154981549815498\n"
     ]
    }
   ],
   "source": [
    "rfc_eval_tvs = BinaryClassificationEvaluator(labelCol='label')\n",
    "print('RFC area under the curve=',rfc_eval_tvs.evaluate(tvsPrediction))\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval_tvs = MulticlassClassificationEvaluator(labelCol='label', metricName='accuracy')\n",
    "rfc_acc_tvs = acc_eval_tvs.evaluate(tvsPrediction)\n",
    "print('rfc acc after tuning= ',rfc_acc_tvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel (uid=rfc_93b4f3d007f5) with 20 trees"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_tvs = tvsModel.bestModel\n",
    "best_model_tvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
