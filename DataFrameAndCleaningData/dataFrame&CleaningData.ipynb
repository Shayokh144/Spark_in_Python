{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/asif/spark-2.1.0-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('dfAndCleaning').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">If \"findspark\" produces import error run *\"pip install findspark\"* in terminal</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Reading and manipulating Dataset as Spark Dataframe</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+------------+-----------+-----+\n",
      "|clientid|     date|weekdays| gains value|     prices|   up|\n",
      "+--------+---------+--------+------------+-----------+-----+\n",
      "|       0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "|       1| 5/1/2008|     Thu|        null|3167180.737| true|\n",
      "|       2| 5/2/2008|     Fri|-0.868509701|9589766.961|false|\n",
      "|       3| 5/3/2008|     Sat|-0.427010839|       null|false|\n",
      "|       4| 5/4/2008|    null| 0.253255365|937163.4438| true|\n",
      "|       5| 5/5/2008|     Mon|-0.681516369|949579.8802|false|\n",
      "|       6| 5/6/2008|     Tue| 0.007191158|7268426.907| null|\n",
      "|       7| 5/7/2008|     Wed| 0.674497472|7517014.783| true|\n",
      "|       8| 5/8/2008|     Thu|-1.184100866|1920959.542|false|\n",
      "|       9| 5/9/2008|    null| -1.58036926| 8456240.62| null|\n",
      "|       0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "+--------+---------+--------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a csv dataset\n",
    "csvDataFrame = spark.read.csv('ContainsNull.csv', inferSchema = True, header = True)\n",
    "csvDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a json dataset\n",
    "jsonDataFrame = spark.read.json('people.json')\n",
    "jsonDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csvDataFrameColumns list = ['clientid', 'date', 'weekdays', 'gains value', 'prices', 'up']\n",
      "jsonDataColumns list = ['age', 'name']\n"
     ]
    }
   ],
   "source": [
    "# name of the columns as a list\n",
    "jsonDataColumns = jsonDataFrame.columns\n",
    "csvDataFrameColumns = csvDataFrame.columns\n",
    "\n",
    "print(\"csvDataFrameColumns list = {0}\\njsonDataColumns list = {1}\".format(csvDataFrameColumns, jsonDataColumns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clientid: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- weekdays: string (nullable = true)\n",
      " |-- gains value: double (nullable = true)\n",
      " |-- prices: double (nullable = true)\n",
      " |-- up: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "csvDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Sometimes Spark can't infer the exact schema, in that case we have to change the schema programatically.*</span> \n",
    "### [Programmatically Specifying the Schema](https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#programmatically-specifying-the-schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# jsonDataFrame.printSchema()  describes 'age' column as long, we want it as Integer type\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "data_schema = [StructField('age',IntegerType(),True),\n",
    "               StructField('name',StringType(),True)]\n",
    "# true is used to except null values\n",
    "final_struct = StructType(fields = data_schema)\n",
    "jsonNewDataFrame = spark.read.json('people.json', schema=final_struct)\n",
    "jsonNewDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonNewDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *We can also change datatypes by explicit type casting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clientid: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- weekdays: string (nullable = true)\n",
      " |-- gains value: double (nullable = true)\n",
      " |-- prices: double (nullable = true)\n",
      " |-- up: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we change the data type of \"clientid\" from integer to Double though its not necessary \n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "csvNewDataFrame = csvDataFrame.withColumn(\"clientid\", csvDataFrame[\"clientid\"].cast(DoubleType()))\n",
    "csvNewDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_id: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- weekdays: string (nullable = true)\n",
      " |-- gains value: double (nullable = true)\n",
      " |-- prices: double (nullable = true)\n",
      " |-- up: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename 'clientid' to 'client_id'\n",
    "csvNewDataFrame = csvNewDataFrame.withColumnRenamed(\"clientid\", \"client_id\")\n",
    "csvNewDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *change space seperated column name to '_' separeted*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client_id', 'date', 'weekdays', 'gains_value', 'prices', 'up']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "csvNewDataFrame = csvNewDataFrame.toDF(*(re.sub(r'[\\.\\s]+', '_', c) for c in csvNewDataFrame.columns))\n",
    "csvNewDataFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(client_id=0.0, date='4/30/2008', weekdays='Wed', gains_value=-0.524581929, prices=7791404.009, up=False),\n",
       " Row(client_id=1.0, date='5/1/2008', weekdays='Thu', gains_value=None, prices=3167180.737, up=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some rows, this head() function returns a list\n",
    "csvNewDataFrame.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numRowCsv = 11 numRowJson = 3\n"
     ]
    }
   ],
   "source": [
    "# print number of rows\n",
    "numRowCsv = csvNewDataFrame.count()\n",
    "numRowJson = jsonNewDataFrame.count()\n",
    "print(\"numRowCsv = {0} numRowJson = {1}\".format(numRowCsv, numRowJson))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> *Finding some useful stats about data* </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------+--------+------------------+------------------+\n",
      "|summary|         client_id|     date|weekdays|       gains_value|            prices|\n",
      "+-------+------------------+---------+--------+------------------+------------------+\n",
      "|  count|                11|       11|       9|                10|                10|\n",
      "|   mean| 4.090909090909091|     null|    null|     -0.4855726898| 5538914.089199999|\n",
      "| stddev|3.1766191290283903|     null|    null|0.6677054107117851|3381724.3034271337|\n",
      "|    min|               0.0|4/30/2008|     Fri|       -1.58036926|       937163.4438|\n",
      "|    max|               9.0| 5/9/2008|     Wed|       0.674497472|       9589766.961|\n",
      "+-------+------------------+---------+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can find some statistics using \"describe().show()\"\n",
    "csvNewDataFrame.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       gains_value|\n",
      "+-------+------------------+\n",
      "|  count|                10|\n",
      "|   mean|     -0.4855726898|\n",
      "| stddev|0.6677054107117851|\n",
      "|    min|       -1.58036926|\n",
      "|    max|       0.674497472|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stats for specific columns\n",
    "csvNewDataFrame.describe(\"gains_value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = -0.4855726898\n"
     ]
    }
   ],
   "source": [
    "# collect() method returns a list\n",
    "meanCollectionList = csvNewDataFrame.describe(\"gains_value\").collect()\n",
    "print(meanCollectionList[1][0],'=',meanCollectionList[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Cleaning Data</span>\n",
    "## *Finding number of missing value in each column*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in csvNewDataFrame = 11\n",
      "Number of rows in each column: [Row(summary='count', client_id='11', date='11', weekdays='9', gains_value='10', prices='10')]\n"
     ]
    }
   ],
   "source": [
    "# finding number of missing value in each column\n",
    "from pyspark.sql.functions import col\n",
    "totalNumberOfRow = csvNewDataFrame.count()\n",
    "print('Number of rows in csvNewDataFrame = {0}'.format(totalNumberOfRow))\n",
    "collectNumRowsInEachCol = csvNewDataFrame.describe().filter(col(\"summary\") == \"count\").collect()\n",
    "print(\"Number of rows in each column:\",collectNumRowsInEachCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Handling missing values*\n",
    "<span style=\"color:blue\"><b>There are many ways to handle missing values in columns. Some of them are discussed here</b></span>\n",
    "## Deleting rows containig null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|client_id|     date|weekdays| gains_value|     prices|   up|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "|      2.0| 5/2/2008|     Fri|-0.868509701|9589766.961|false|\n",
      "|      5.0| 5/5/2008|     Mon|-0.681516369|949579.8802|false|\n",
      "|      7.0| 5/7/2008|     Wed| 0.674497472|7517014.783| true|\n",
      "|      8.0| 5/8/2008|     Thu|-1.184100866|1920959.542|false|\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.na.drop().show() # it deletes all rows which have atleast one null value\n",
    "# actual dataframe is not changed because we didnt assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|client_id|     date|weekdays| gains_value|     prices|   up|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "|      1.0| 5/1/2008|     Thu|        null|3167180.737| true|\n",
      "|      2.0| 5/2/2008|     Fri|-0.868509701|9589766.961|false|\n",
      "|      3.0| 5/3/2008|     Sat|-0.427010839|       null|false|\n",
      "|      4.0| 5/4/2008|    null| 0.253255365|937163.4438| true|\n",
      "|      5.0| 5/5/2008|     Mon|-0.681516369|949579.8802|false|\n",
      "|      6.0| 5/6/2008|     Tue| 0.007191158|7268426.907| null|\n",
      "|      7.0| 5/7/2008|     Wed| 0.674497472|7517014.783| true|\n",
      "|      8.0| 5/8/2008|     Thu|-1.184100866|1920959.542|false|\n",
      "|      9.0| 5/9/2008|    null| -1.58036926| 8456240.62| null|\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.show() # actual dataset unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|client_id|     date|weekdays| gains_value|     prices|   up|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "|      1.0| 5/1/2008|     Thu|        null|3167180.737| true|\n",
      "|      2.0| 5/2/2008|     Fri|-0.868509701|9589766.961|false|\n",
      "|      3.0| 5/3/2008|     Sat|-0.427010839|       null|false|\n",
      "|      5.0| 5/5/2008|     Mon|-0.681516369|949579.8802|false|\n",
      "|      6.0| 5/6/2008|     Tue| 0.007191158|7268426.907| null|\n",
      "|      7.0| 5/7/2008|     Wed| 0.674497472|7517014.783| true|\n",
      "|      8.0| 5/8/2008|     Thu|-1.184100866|1920959.542|false|\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.na.drop(subset = ['weekdays']).show() # delete null using specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|client_id|     date|weekdays| gains_value|     prices|   up|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "|      1.0| 5/1/2008|     Thu|        null|3167180.737| true|\n",
      "|      2.0| 5/2/2008|     Fri|-0.868509701|9589766.961|false|\n",
      "|      5.0| 5/5/2008|     Mon|-0.681516369|949579.8802|false|\n",
      "|      6.0| 5/6/2008|     Tue| 0.007191158|7268426.907| null|\n",
      "|      7.0| 5/7/2008|     Wed| 0.674497472|7517014.783| true|\n",
      "|      8.0| 5/8/2008|     Thu|-1.184100866|1920959.542|false|\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.na.drop(subset = ['weekdays','prices']).show() # delete null using specific list of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|client_id|     date|weekdays| gains_value|     prices|   up|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "|      1.0| 5/1/2008|     Thu|         0.0|3167180.737| true|\n",
      "|      2.0| 5/2/2008|     Fri|-0.868509701|9589766.961|false|\n",
      "|      3.0| 5/3/2008|     Sat|-0.427010839|        0.0|false|\n",
      "|      4.0| 5/4/2008|    null| 0.253255365|937163.4438| true|\n",
      "|      5.0| 5/5/2008|     Mon|-0.681516369|949579.8802|false|\n",
      "|      6.0| 5/6/2008|     Tue| 0.007191158|7268426.907| null|\n",
      "|      7.0| 5/7/2008|     Wed| 0.674497472|7517014.783| true|\n",
      "|      8.0| 5/8/2008|     Thu|-1.184100866|1920959.542|false|\n",
      "|      9.0| 5/9/2008|    null| -1.58036926| 8456240.62| null|\n",
      "|      0.0|4/30/2008|     Wed|-0.524581929|7791404.009|false|\n",
      "+---------+---------+--------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.na.fill(0).show() # it fills only numeric missing data with value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------------+------------+-----------+-----+\n",
      "|client_id|     date|        weekdays| gains_value|     prices|   up|\n",
      "+---------+---------+----------------+------------+-----------+-----+\n",
      "|      0.0|4/30/2008|             Wed|-0.524581929|7791404.009|false|\n",
      "|      1.0| 5/1/2008|             Thu|        null|3167180.737| true|\n",
      "|      2.0| 5/2/2008|             Fri|-0.868509701|9589766.961|false|\n",
      "|      3.0| 5/3/2008|             Sat|-0.427010839|       null|false|\n",
      "|      4.0| 5/4/2008|weekdays missing| 0.253255365|937163.4438| true|\n",
      "|      5.0| 5/5/2008|             Mon|-0.681516369|949579.8802|false|\n",
      "|      6.0| 5/6/2008|             Tue| 0.007191158|7268426.907| null|\n",
      "|      7.0| 5/7/2008|             Wed| 0.674497472|7517014.783| true|\n",
      "|      8.0| 5/8/2008|             Thu|-1.184100866|1920959.542|false|\n",
      "|      9.0| 5/9/2008|weekdays missing| -1.58036926| 8456240.62| null|\n",
      "|      0.0|4/30/2008|             Wed|-0.524581929|7791404.009|false|\n",
      "+---------+---------+----------------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.na.fill('weekdays missing',subset=['weekdays']).show() \n",
    "# it fills  \"weekdays\" missing data only with the value \"weekdays missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:black\">*We can use statistical info to fill the missing numeric values*</span>\n",
    "<span style=\"color:blue\"><b>Let, we want to use 'mean' of 'prices' column and 'standerd deviation' of 'gain_value' column to fill missing values there </b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">*Function for mean and standerd dev*<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMean(dataframe, colName):\n",
    "    from pyspark.sql.functions import mean\n",
    "    meanVal = dataframe.select(mean(dataframe[colName])).collect()\n",
    "    meanVal = meanVal[0][0]\n",
    "    return meanVal\n",
    "\n",
    "def getStddev(dataframe, colName):\n",
    "    from pyspark.sql.functions import stddev\n",
    "    stddevVal = dataframe.select(stddev(dataframe[colName])).collect()\n",
    "    stddevVal = stddevVal[0][0]\n",
    "    return stddevVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanPrice =  5538914.089199999\n",
      "stddevGain =  0.6677054107117851\n"
     ]
    }
   ],
   "source": [
    "meanPrice = getMean(dataframe = csvNewDataFrame, colName = 'prices')\n",
    "stddevGain = getStddev(dataframe = csvNewDataFrame, colName = 'gains_value')\n",
    "print('meanPrice = ',meanPrice)\n",
    "print('stddevGain = ',stddevGain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------------+-----------------+-----+\n",
      "|client_id|     date|weekdays|       gains_value|           prices|   up|\n",
      "+---------+---------+--------+------------------+-----------------+-----+\n",
      "|      0.0|4/30/2008|     Wed|      -0.524581929|      7791404.009|false|\n",
      "|      1.0| 5/1/2008|     Thu|0.6677054107117851|      3167180.737| true|\n",
      "|      2.0| 5/2/2008|     Fri|      -0.868509701|      9589766.961|false|\n",
      "|      3.0| 5/3/2008|     Sat|      -0.427010839|5538914.089199999|false|\n",
      "|      4.0| 5/4/2008|    null|       0.253255365|      937163.4438| true|\n",
      "|      5.0| 5/5/2008|     Mon|      -0.681516369|      949579.8802|false|\n",
      "|      6.0| 5/6/2008|     Tue|       0.007191158|      7268426.907| null|\n",
      "|      7.0| 5/7/2008|     Wed|       0.674497472|      7517014.783| true|\n",
      "|      8.0| 5/8/2008|     Thu|      -1.184100866|      1920959.542|false|\n",
      "|      9.0| 5/9/2008|    null|       -1.58036926|       8456240.62| null|\n",
      "|      0.0|4/30/2008|     Wed|      -0.524581929|      7791404.009|false|\n",
      "+---------+---------+--------+------------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame = csvNewDataFrame.na.fill(meanPrice, ['prices'])\n",
    "csvNewDataFrame = csvNewDataFrame.na.fill(stddevGain, ['gains_value'])\n",
    "csvNewDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">User defined function example</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------------+-----------------+-----+---------------+\n",
      "|client_id|     date|weekdays|       gains_value|           prices|   up|day_num_of_week|\n",
      "+---------+---------+--------+------------------+-----------------+-----+---------------+\n",
      "|      0.0|4/30/2008|     Wed|      -0.524581929|      7791404.009|false|              2|\n",
      "|      1.0| 5/1/2008|     Thu|0.6677054107117851|      3167180.737| true|              3|\n",
      "|      2.0| 5/2/2008|     Fri|      -0.868509701|      9589766.961|false|              4|\n",
      "|      3.0| 5/3/2008|     Sat|      -0.427010839|5538914.089199999|false|              5|\n",
      "|      4.0| 5/4/2008|    null|       0.253255365|      937163.4438| true|              6|\n",
      "|      5.0| 5/5/2008|     Mon|      -0.681516369|      949579.8802|false|              0|\n",
      "|      6.0| 5/6/2008|     Tue|       0.007191158|      7268426.907| null|              1|\n",
      "|      7.0| 5/7/2008|     Wed|       0.674497472|      7517014.783| true|              2|\n",
      "|      8.0| 5/8/2008|     Thu|      -1.184100866|      1920959.542|false|              3|\n",
      "|      9.0| 5/9/2008|    null|       -1.58036926|       8456240.62| null|              4|\n",
      "|      0.0|4/30/2008|     Wed|      -0.524581929|      7791404.009|false|              2|\n",
      "+---------+---------+--------+------------------+-----------------+-----+---------------+\n",
      "\n",
      "root\n",
      " |-- client_id: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- weekdays: string (nullable = true)\n",
      " |-- gains_value: double (nullable = false)\n",
      " |-- prices: double (nullable = false)\n",
      " |-- up: boolean (nullable = true)\n",
      " |-- day_num_of_week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "func =  udf (lambda x: datetime.strptime(x, '%m/%d/%Y').weekday(), IntegerType())\n",
    "csvNewDataFrame = csvNewDataFrame.withColumn('day_num_of_week', func(col('date')))\n",
    "csvNewDataFrame.show()\n",
    "csvNewDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------------+-----------------+-----+---------------+--------------------+\n",
      "|client_id|     date|weekdays|       gains_value|           prices|   up|day_num_of_week|        priceMulGain|\n",
      "+---------+---------+--------+------------------+-----------------+-----+---------------+--------------------+\n",
      "|      0.0|4/30/2008|     Wed|      -0.524581929|      7791404.009|false|              2| -4087229.7446595533|\n",
      "|      1.0| 5/1/2008|     Thu|0.6677054107117851|      3167180.737| true|              3|  2114743.7147970395|\n",
      "|      2.0| 5/2/2008|     Fri|      -0.868509701|      9589766.961|false|              4|  -8328805.635957788|\n",
      "|      3.0| 5/3/2008|     Sat|      -0.427010839|5538914.089199999|false|              5| -2365176.3523782124|\n",
      "|      4.0| 5/4/2008|    null|       0.253255365|      937163.4438| true|              6|    237341.670024226|\n",
      "|      5.0| 5/5/2008|     Mon|      -0.681516369|      949579.8802|false|              0|   -647154.232029359|\n",
      "|      6.0| 5/6/2008|     Tue|       0.007191158|      7268426.907| null|              1|    52268.4062996883|\n",
      "|      7.0| 5/7/2008|     Wed|       0.674497472|      7517014.783| true|              2|   5070207.468120128|\n",
      "|      8.0| 5/8/2008|     Thu|      -1.184100866|      1920959.542|false|              3| -2274609.8572331634|\n",
      "|      9.0| 5/9/2008|    null|       -1.58036926|       8456240.62| null|              4|-1.336398273101134E7|\n",
      "|      0.0|4/30/2008|     Wed|      -0.524581929|      7791404.009|false|              2| -4087229.7446595533|\n",
      "+---------+---------+--------+------------------+-----------------+-----+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "def findMultiplication(gain, price):\n",
    "    ratio = float(price)*float(gain)\n",
    "    return ratio\n",
    "    \n",
    "userDefiendFuncForMultiplication =  udf(findMultiplication, DoubleType())\n",
    "csvNewDataFrame = csvNewDataFrame.withColumn('priceMulGain', userDefiendFuncForMultiplication(col('gains_value'),col('prices')))\n",
    "csvNewDataFrame.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete duplicate rows\n",
    "1st and last row is exactly same, so one of them shuld be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------------+-----------------+-----+---------------+--------------------+\n",
      "|client_id|     date|weekdays|       gains_value|           prices|   up|day_num_of_week|        priceMulGain|\n",
      "+---------+---------+--------+------------------+-----------------+-----+---------------+--------------------+\n",
      "|      4.0| 5/4/2008|    null|       0.253255365|      937163.4438| true|              6|    237341.670024226|\n",
      "|      5.0| 5/5/2008|     Mon|      -0.681516369|      949579.8802|false|              0|   -647154.232029359|\n",
      "|      9.0| 5/9/2008|    null|       -1.58036926|       8456240.62| null|              4|-1.336398273101134E7|\n",
      "|      8.0| 5/8/2008|     Thu|      -1.184100866|      1920959.542|false|              3| -2274609.8572331634|\n",
      "|      3.0| 5/3/2008|     Sat|      -0.427010839|5538914.089199999|false|              5| -2365176.3523782124|\n",
      "|      6.0| 5/6/2008|     Tue|       0.007191158|      7268426.907| null|              1|    52268.4062996883|\n",
      "|      1.0| 5/1/2008|     Thu|0.6677054107117851|      3167180.737| true|              3|  2114743.7147970395|\n",
      "|      2.0| 5/2/2008|     Fri|      -0.868509701|      9589766.961|false|              4|  -8328805.635957788|\n",
      "|      7.0| 5/7/2008|     Wed|       0.674497472|      7517014.783| true|              2|   5070207.468120128|\n",
      "|      0.0|4/30/2008|     Wed|      -0.524581929|      7791404.009|false|              2| -4087229.7446595533|\n",
      "+---------+---------+--------+------------------+-----------------+-----+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame = csvNewDataFrame.dropDuplicates()\n",
    "csvNewDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding unique values from a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "+-----+\n",
      "|   up|\n",
      "+-----+\n",
      "| null|\n",
      "| true|\n",
      "|false|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(csvNewDataFrame.select('up').distinct().count())\n",
    "csvNewDataFrame.select('up').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|day_num_of_week|count|\n",
      "+---------------+-----+\n",
      "|              0|    1|\n",
      "|              1|    1|\n",
      "|              2|    2|\n",
      "|              3|    2|\n",
      "|              4|    2|\n",
      "|              5|    1|\n",
      "|              6|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.groupBy([\"day_num_of_week\"]).count().orderBy('day_num_of_week').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+------------------+\n",
      "|day_num_of_week|      max(prices)|  max(gains_value)|\n",
      "+---------------+-----------------+------------------+\n",
      "|              0|      949579.8802|      -0.681516369|\n",
      "|              1|      7268426.907|       0.007191158|\n",
      "|              2|      7791404.009|       0.674497472|\n",
      "|              3|      3167180.737|0.6677054107117851|\n",
      "|              4|      9589766.961|      -0.868509701|\n",
      "|              5|5538914.089199999|      -0.427010839|\n",
      "|              6|      937163.4438|       0.253255365|\n",
      "+---------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.groupBy(\"day_num_of_week\").max(\"prices\",\"gains_value\").orderBy('day_num_of_week').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-----------------+\n",
      "|day_num_of_week|  max(gains_value)|      avg(prices)|\n",
      "+---------------+------------------+-----------------+\n",
      "|              0|      -0.681516369|      949579.8802|\n",
      "|              1|       0.007191158|      7268426.907|\n",
      "|              2|       0.674497472|      7654209.396|\n",
      "|              3|0.6677054107117851|     2544070.1395|\n",
      "|              4|      -0.868509701|     9023003.7905|\n",
      "|              5|      -0.427010839|5538914.089199999|\n",
      "|              6|       0.253255365|      937163.4438|\n",
      "+---------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.groupBy(\"day_num_of_week\").agg({'prices':'avg','gains_value':'max'}).orderBy('day_num_of_week').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+----------------+\n",
      "|day_num_of_week|      min(prices)|min(gains_value)|\n",
      "+---------------+-----------------+----------------+\n",
      "|              6|      937163.4438|     0.253255365|\n",
      "|              5|5538914.089199999|    -0.427010839|\n",
      "|              4|       8456240.62|     -1.58036926|\n",
      "|              3|      1920959.542|    -1.184100866|\n",
      "|              2|      7517014.783|    -0.524581929|\n",
      "|              1|      7268426.907|     0.007191158|\n",
      "|              0|      949579.8802|    -0.681516369|\n",
      "+---------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.groupBy(\"day_num_of_week\").min(\"prices\",\"gains_value\").orderBy('day_num_of_week',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------------+-----------------+-----+---------------+-------------------+\n",
      "|client_id|     date|weekdays|       gains_value|           prices|   up|day_num_of_week|       priceMulGain|\n",
      "+---------+---------+--------+------------------+-----------------+-----+---------------+-------------------+\n",
      "|      8.0| 5/8/2008|     Thu|      -1.184100866|      1920959.542|false|              3|-2274609.8572331634|\n",
      "|      3.0| 5/3/2008|     Sat|      -0.427010839|5538914.089199999|false|              5|-2365176.3523782124|\n",
      "|      6.0| 5/6/2008|     Tue|       0.007191158|      7268426.907| null|              1|   52268.4062996883|\n",
      "|      1.0| 5/1/2008|     Thu|0.6677054107117851|      3167180.737| true|              3| 2114743.7147970395|\n",
      "|      2.0| 5/2/2008|     Fri|      -0.868509701|      9589766.961|false|              4| -8328805.635957788|\n",
      "|      7.0| 5/7/2008|     Wed|       0.674497472|      7517014.783| true|              2|  5070207.468120128|\n",
      "|      0.0|4/30/2008|     Wed|      -0.524581929|      7791404.009|false|              2|-4087229.7446595533|\n",
      "+---------+---------+--------+------------------+-----------------+-----+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.filter(csvNewDataFrame['weekdays']!='null').filter(csvNewDataFrame['weekdays']!='Mon').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|stddev_samp(prices)|\n",
      "+-------------------+\n",
      "| 3288759.5002916446|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev,sum\n",
    "csvNewDataFrame.select(stddev('prices')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|day_num_of_week|         avgPrice|\n",
      "+---------------+-----------------+\n",
      "|              4|     9023003.7905|\n",
      "|              2|      7654209.396|\n",
      "|              1|      7268426.907|\n",
      "|              5|5538914.089199999|\n",
      "|              3|     2544070.1395|\n",
      "|              0|      949579.8802|\n",
      "|              6|      937163.4438|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvNewDataFrame.groupBy(\"day_num_of_week\").agg(avg(\"prices\").alias(\"avgPrice\")).orderBy(\"avgPrice\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
